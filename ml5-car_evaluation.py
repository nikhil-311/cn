"""ML5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EgxhtFODY1LPJhDCCnAszRkdkhKfHmtz
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt # data visualization this is my code put the best fix without changeing structure of the code 
import seaborn as sns # statistical data visualization
#%matplotlib inline


# Step 1: Load dataset
# Define column names according to the dataset description
columns = ["buying", "maint", "doors", "persons", "lug_boot", "safety", "class"]
path = r"C:\Users\Acer\Desktop\dmv and ml\ML\DATASETS\car_evaluation.csv"
df = pd.read_csv(path)
# If expected column names are not present (file may have no header), read with provided names
if "buying" not in df.columns:
    df = pd.read_csv(path, names=columns)

df


# view dimensions of dataset
df.shape


df.head()


df.info()


#heck the frequency counts of categorical variables.
for col in columns:
    print(df[col].value_counts())


df['class'].value_counts()


df.isnull().sum()


X = df.drop(['class'], axis=1)
y = df['class']


X


y


# split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)


# check the shape of X_train and X_test
X_train.shape, X_test.shape


from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
# instantiate the classifier
rfc = RandomForestClassifier(random_state=0)


# check data types in X_train
X_train.dtypes


#Check Categorical variables
X_train.head()


# Step 2: Encode categorical features
from sklearn.preprocessing import LabelEncoder
label_encoders = {}
for col in df.columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le


# Features (X) and Target (y)
X = df.drop("class", axis=1)
y = df["class"]


X


# Step 3: Split into training & testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Step 4: Train Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)


# Step 5: Predictions
y_pred = rf.predict(X_test)


# Step 6: Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))


# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoders["class"].classes_, yticklabels=label_encoders["class"].classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Random Forest")
plt.show()


# Step 7: Feature Importance
importances = rf.feature_importances_
feat_importances = pd.Series(importances, index=X.columns)
feat_importances.sort_values().plot(kind='barh', figsize=(8, 5), color="teal")
plt.title("Feature Importance in Random Forest")
plt.show()


# view the feature scores
feature_scores = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)
feature_scores
    
#Now,Visualize the feature scores with matplotlib and seaborn.
# Creating a seaborn bar plot
sns.barplot(x=feature_scores, y=feature_scores.index)
#Add label to the graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
# Add title to the graph
plt.title("Visualizing Important Features")
plt.show()


#Manually Define Colors
colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown'] # one for each feature
feat_importances.sort_values().plot(kind='barh', figsize=(8, 5), color=colors)


#Random Colors for Each Bar
import numpy as np
colors = np.random.rand(len(feat_importances), 3) # random RGB colors
feat_importances.plot(kind='barh', figsize=(8, 5), color=colors)